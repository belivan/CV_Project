PLAN OUTLINE:
SETUP:
1. DATA (extract_data.py) (extract_origin_video.py)
Extracted data from source and saved color frames (600+) as .png and depth as .npy.
	- found intrinsic parameters
	- made sure its synced with iphone and vise versa
	
ANALYSIS:
2. Imported the data (.png and .npy) 
3. Identify Helen and pose in the color images && Create a collection of 2D image pixel points
3. Using depth and intrinsic parameters, translate image pixel coordinates (u,v) to (x,y,z)

TRANSFORM
4. Using 3D coordinates, transform them from RealSense coordinate system to iPhone coordinate system.
5. Display the transformed points on the iPhone data (.png)
6. Display the frames from the two perspectives side by side and perform visual assesment.


SCRIPTS INCLUDED:

DATA_MANAGER.py : convenient tool for accessing all the data (real sense color/depth && iPhone)
 - all data is synchronized and sorted
 - depth data is filtered (removed outliers, was necessary)

PIXEL_TO_3D.py : tool for converting pixels vals to 3D
 - useful when identifying objects and need to convert them to 3D

CAMERA_MERGE.py : tool for displaying final project results
 - displays iphone && intel realsense as video streams
 - (optional) save a MP4 video if file name/path provided
 - has demo code inside for a demo video
